---
title: Библиотека квантового машинного обучения
author: alexeib2
ms.author: alexei.bocharov@microsoft.com
ms.date: 11/22/2019
ms.topic: article
uid: microsoft.quantum.libraries.machine-learning.introduction
ms.openlocfilehash: 4c42612fee3a58e15368677bb2c77a70c5680f45
ms.sourcegitcommit: 0181e7c9e98f9af30ea32d3cd8e7e5e30257a4dc
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 06/23/2020
ms.locfileid: "85276173"
---
# <a name="introduction-to-quantum-machine-learning"></a>Введение в тактовую Машинное обучение

## <a name="framework-and-goals"></a>Платформа и цели

Кодирование и обработка данных в такте — это мощная альтернатива классическим классам-классификаторам такта машинного обучения, в частности, кодирования данных в тактовых регистрах, которые кратко относятся к числу функций, систематически применяют тактовую замкнутые в качестве вычислительного ресурса и используют измерение тактов для вывода класса.
Классификатор такта, ориентированный на канал, — это относительно простое решение, объединяющее кодировку данных с быстрой ентанглинг/распутывание запутанной сети тактовой цепью, за которым следует измерение для определения меток классов для выборок данных.
Цель состоит в том, чтобы обеспечить классический механизм распознавания и хранение цепей предметной области, а также гибридный такт или классический обучающий курс параметров канала даже для очень больших функциональных пространств.

## <a name="classifier-architecture"></a>Архитектура классификатора

Классификация — это задача защищенного машинного обучения, где целью является определение меток классов $ \{ y_1, y_2, \лдотс, y_d \} $ некоторых примеров данных. "Обучающий набор данных" — это коллекция примеров $ \Маскал{д} = \{ (x, y)} $ с известными предварительно назначенными метками. Здесь $x $ является образцом данных и $y $ — это известная метка с названием «обучающая метка».
По аналогии с традиционными методами, классификация тактов состоит из трех этапов:
- кодирование данных
- подготовка состояния классификатора
- измерения из-за природы вероятностная измерения, эти три шага должны повторяться несколько раз. Измерение можно просмотреть в виде тактового эквивалента нелинейной активации.
Как кодирование, так и вычисления состояния классификатора выполняются с помощью *цепей тактов*. Несмотря на то, что канал кодирования обычно управляется данными и без параметров, цепь-классификатор содержит достаточный набор необходимых для изучения параметров. 

В предложенном решении цепь-классификатор состоит из однокубитных поворотов и кубитных поворотов. Для изучения этих параметров используются углы поворота. Шлюзы ротации и управления, управляемые, являются *универсальными* для вычислений тактов. Это означает, что любую одновесовую матрицу можно разложить на достаточно длинный канал, состоящий из таких шлюзов.

![Многоуровневый перцептрона и классификатор, ориентированный на канал](~/media/DLvsQCC.png)

Мы можем сравнить эту модель с многоуровневой перцептронаой, чтобы получить более полное представление о базовой структуре. В перцептрона прогнозируемый $p (y | x, \сета) $ является параметризованным по набору весов $ \сета $, который определяет линейные функции, соединяющие функции нелинейной активации (нейроны). Эти параметры можно обучить для создания модели. На выходном слое можно получить вероятность выборки, принадлежащей классу, с помощью нелинейных функций активации, таких как softmax. В классификаторе, ориентированном на канал, прогнозируемый параметризованным с помощью углов вращения, расположенных на поворотах одного кубит и двух кубит, управляемых в цепи моделей. Аналогичным образом эти параметры можно обучить с помощью гибридного такта или классической версии алгоритма спуска градиента. Чтобы вычислить выходные данные вместо использования нелинейных функций активации, вероятность класса достигается путем считывания повторяющихся измерений по определенному кубит после управляемого вращения. Для кодирования классических данных в состоянии такта мы используем управляемый канал кодирования для подготовки состояния.

В нашей архитектуре рассматриваются относительно неполные каналы, которые, следовательно, должны быть *быстро ентанглинг* для захвата всех корреляций между функциями данных во всех диапазонах. Пример наиболее полезного компонента ентанглинг цепи показан на рисунке ниже. Несмотря на то, что канал с этой геометрической схемой состоит только из $3 n + 1 $ Gates, формируемая им матрица единого веса обеспечивает значительный обмен данными между компонентами $2 ^ n $.

![Быстро ентанглинг тактовую цепь на 5 Кубитс (с двумя циклическими слоями).](~/media/5-qubit-qccc.png)

Цепь в приведенном выше примере состоит из 6 однокубитных шлюзов $ (G_1, \лдотс, G_5; G_ {16} ) $ и 10 2-Кубитс Gates $ (G_6, \лдотс, G_ {15} ) $. Предполагая, что каждый из шлюзов определен с одним из этих параметров, у нас есть 16 подученных параметров, а измерение 5-кубит Гильберта пространство — 32. Такая геометрия канала может быть легко обобщена любому $n $-кубит регистру, когда $n $ является нечетным, что дает цепи с $3 n + 1 $ параметрами для $2 ^ n $-многомерного пространства.

## <a name="classifier-training-as-a-supervised-learning-task"></a>Обучение классификатора как контролируемая задача обучения

Обучение модели-классификатора включает в себя поиск оптимальных значений его операционных параметров, что позволяет максимально увеличить среднюю вероятность получения правильных меток обучения по обучающим образцам.
Здесь мы будем беспокоиться только о двух уровнях классификации, т. е. в случае $d = $2 и только два класса с метками $y _1, y_2 $.

> [!NOTE]
> Способ обобщения наших методов с произвольным числом классов заключается в замене Кубитс на кудитс, т. е. единицами такта с $d $ базисными состояниями и двусторонним измерением с $d $-Way.

### <a name="likelihood-as-the-training-goal"></a>Правдоподобие цели обучения

Учитывая проученную цепь тактовой $U (\сета) $, где $ \сета $ является вектором параметров и обозначая окончательную меру на $M $, средняя вероятность правильного вывода метки — $ $ \бегин{алигн} \Маскал{л} (\сета) = \фрак {1} {| \маскал{д} |} \лефт (\ sum_ {(x, y_1) \Ин\маскал{д}} P (M = y_1 | U (\сета) x) + \ sum_ {(x, y_2) \Ин\маскал{д}} P (M = y_2 | U (\сета) x) \ригхт) \енд{алигн} $ $ WHERE $P (M = y | z) $ является вероятностью измерения $y $ в состоянии такта $z $.
Здесь достаточно понять, что функция правдоподобия $ \Маскал{л} (\сета) $ является гладким в $ \сета $ и ее производная в любом $ \ theta_j $ может быть вычислена по сути тем же протоколом такта, который используется для вычисления самой функции правдоподобия. Это позволяет оптимизировать спуск шкалы $ \Маскал{л} (\сета) $ по градиенту.

### <a name="classifier-bias-and-training-score"></a>Смещение классификатора и оценка курса обучения

Учитывая некоторые промежуточные (или окончательные) значения параметров в $ \сета $, нам нужно определить одно вещественное значение $b $ знать как *классификатор смещения* для выполнения вывода. Правило вывода метки работает следующим образом: 
- Пример $x $ назначается меткой $y _2 $ if и только в том случае, если $P (M = y_2 | U (\сета) x) + b > $0,5 (RULE1) (в противном случае назначается метка $y _1 $)

Явно $b $ должен быть в пределах $ (-0,5, + 0,5) $, чтобы быть осмысленным.

Обучающий вариант $ (x, y) \ин \Маскал{д} $ считается неправильной *классификацией* с учетом смещения $b $ если метка, выводимая для $x $ в соответствии с RULE1, на самом деле отличается от $y $. Общее число неправильной классификации — это *показатель обучения* классификатора, заданный в $b $. *Оптимальный* уровень смещения классификатора $b $ свертывает оценку обучения. Это легко увидеть, учитывая предварительно вычисленные оценки вероятности $ \{ P (M = y_2 | U (\сета) x) | (x, *) \Ин\маскал{д} \} $. оптимальное смещение классификатора можно найти по двоичному поиску в интервале $ (-0,5, + 0,5) $, установив максимум $ \ Log_2 (| \маскал{д} |) $ шаги.

### <a name="reference"></a>Справочные сведения

Эта информация должна быть достаточной для начала воспроизведения кода. Однако если вы хотите узнать больше об этой модели, прочитайте первоначальное предложение: [ *"генераторы тактов на основе цепи", Мария Счулд, Алекс Бочаров, Криста Своре и (Nathan виебе*](https://arxiv.org/abs/1804.00633)
