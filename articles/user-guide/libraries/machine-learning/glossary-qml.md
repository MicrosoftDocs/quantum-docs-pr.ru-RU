---
title: Библиотека квантового машинного обучения
author: alexeib2
ms.author: alexei.bocharov@microsoft.com
ms.date: 2/27/2020
ms.topic: article
uid: microsoft.quantum.libraries.machine-learning.training
ms.openlocfilehash: f9b33a607a892179795d0700ba3080f9a24ab94a
ms.sourcegitcommit: 0181e7c9e98f9af30ea32d3cd8e7e5e30257a4dc
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 06/23/2020
ms.locfileid: "85276209"
---
# <a name="quantum-machine-learning-glossary"></a>Глоссарий Машинное обучение

Обучение классификатора тактовой передачи, ориентированного на цепь, — это процесс с множеством движущихся частей, требующих одинаковых (или немного большего) объема калибровки на основе пробы и ошибок в процессе обучения традиционных классификаторов. Здесь мы определим основные понятия и составляющие этого обучающего процесса.

## <a name="trainingtesting-schedules"></a>Расписания обучения и тестирования

В контексте обучения классификатора описывается подмножество *schedule* примеров данных в общем обучающем или проверочном наборе. Расписание обычно определяется как коллекция образцов индексов.

## <a name="parameterbias-scores"></a>Параметры/показатели смещений

При наличии вектора параметров-кандидата и смещения классификатора их *Оценка* измеряется относительно выбранного расписания проверки и выражается в количестве неправильной классификации по всем примерам в расписании.

## <a name="hyperparameters"></a>Гиперпараметров

Процесс обучения модели регулируется определенными предварительно заданными значениями, которые называются *параметрами*.

### <a name="learning-rate"></a>Скорость обучения

Это один из ключевых параметров. Он определяет, сколько текущая оценка метод стохастического градиента влияет на обновление параметра. Размер разностного обновления параметра пропорционален скорости обучения. Меньшие значения скорости обучения приводят к более медленному развитию параметров и более медленному конвергенции, но чрезмерно большие значения в LR могут нарушить схождение, так как спуск градиента никогда не фиксируется на определенном локальном минимуме. Хотя курс обучения адаптируется к некоторой степени в соответствии с алгоритмом обучения, для него важно выбрать хорошее начальное значение. Обычно начальное значение по умолчанию для курса обучения — 0,1. Выбор лучшего значения курса обучения — это тонкая схема (см. раздел 4,3 из Гудфеллов et al., "глубокое обучение", нажмите кнопку MIT, 2017).

### <a name="minibatch-size"></a>Размер уменьшив

Определяет, сколько выборок данных используется для одной оценки метод стохастического градиента. Большие значения размера уменьшив обычно приводят к более надежному и монотонному конвергенции, но может замедлить процесс обучения, так как затраты на одну оценку градиента пропорциональны размеру minimatch. Обычно значение по умолчанию для размера уменьшив равно 10.

### <a name="training-epochs-tolerance-gridlocks"></a>Обучающие эпохи, допуск, гридлоккс

«Эпоха» означает один полный проход по запланированным обучающим данным.
Максимальное число эпох на обучающий поток (см. ниже) должно быть ограничено. Поток обучения определяется как завершенный (с наиболее известными параметрами-кандидатами) при выполнении максимального числа эпох. Однако такое обучение должно завершиться раньше, если ставка неправильной классификации в расписании проверки станет ниже выбранной допустимости. Предположим, например, что погрешность неправильной классификации равна 0,01 (1%); Если в проверочном наборе из 2000 обнаружено менее 20 недопустимых классификаций, то достигнут уровень допуска. Поток обучения также завершается преждевременно, если оценка модели-кандидата не показала никакого улучшения по нескольким последовательным эпохам (пробок). Логика завершения пробок в настоящее время жестко закодирована.

### <a name="measurements-count"></a>Количество измерений

Оценка оценок обучения и проверки и компонентов градиента метод стохастического на устройстве такта для оценки перекрытий состояний тактов, требующих нескольких измерений соответствующего observable. Количество единиц измерения должно масштабироваться как $O (1/\ Эпсилон ^ 2) $, где $ \епсилон $ — желаемая ошибка оценки.
Как правило, начальное количество измерений может составлять приблизительно $1/\ Mbox {допуск} ^ 2 $ (см. Определение допуска в предыдущем абзаце). Если шкала градиента кажется слишком неустойчивой и для достижения этого не удается добиться слишком сложного конвергенциа, необходимо будет пересмотреть число измерений вверх.

### <a name="training-threads"></a>Обучающие потоки

Функция правдоподобия, которая является служебной программой для классификатора, очень редко выпуклой, то есть она обычно имеет множество локальных Оптима в пространстве параметров, которая может значительно различаться по качеству. Поскольку процесс SGD может объединять только один конкретный оптимальный способ, важно изучить несколько начальных векторов параметров. В машинном обучении обычно инициализируют такие начальные векторы случайным образом. API обучения Q # принимает произвольный массив таких начальных векторов, но базовый код анализирует их последовательно. На многоядерной компьютере или на любой архитектуре параллельных вычислений рекомендуется выполнять несколько вызовов API-интерфейса Q # Training параллельно с разными инициализациями параметров во всех вызовах.

#### <a name="how-to-modify-the-hyperparameters"></a>Изменение параметров

В библиотеке КМЛ наилучшим способом изменения параметров является переопределение значений определяемого пользователем типа по умолчанию [`TrainingOptions`](xref:microsoft.quantum.machinelearning.trainingoptions) . Для этого мы вызываем его с помощью функции [`DefaultTrainingOptions`](xref:microsoft.quantum.machinelearning.defaulttrainingoptions) и применяем оператор `w/` для переопределения значений по умолчанию. Например, чтобы использовать измерения 100 000 и частоту обучения 0,01:
 ```qsharp
let options = DefaultTrainingOptions()
w/ LearningRate <- 0.01
w/ NMeasurements <- 100000;
 ```
