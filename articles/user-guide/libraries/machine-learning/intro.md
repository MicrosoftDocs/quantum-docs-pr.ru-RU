---
title: Библиотека квантового машинного обучения
author: alexeib2
ms.author: alexei.bocharov@microsoft.com
ms.date: 11/22/2019
ms.topic: article
uid: microsoft.quantum.libraries.machine-learning.intro
no-loc:
- Q#
- $$v
ms.openlocfilehash: 65b0aa6a7f385765933d4d89ce34901f77cf76ec
ms.sourcegitcommit: 75c4edc7c410cc63dc8352e2a5bef44b433ed188
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 08/25/2020
ms.locfileid: "88863096"
---
# <a name="introduction-to-quantum-machine-learning"></a>Введение в тактовую Машинное обучение

## <a name="framework-and-goals"></a>Платформа и цели

Кодирование и обработка данных в такте — это мощная альтернатива классическим классификаторам тактов для машинного обучения. В частности, это позволяет нам кодировать данные в тактовых регистрах, которые кратко относятся к числу функций, систематически применяя тактовый замкнутые в качестве вычислительного ресурса и применяют измерение тактов для вывода класса.
Классификатор такта, ориентированный на канал, — это относительно простое решение, объединяющее кодировку данных с быстрой ентанглинг/распутывание запутанной сети тактовой цепью, за которым следует измерение для определения меток классов для выборок данных.
Цель состоит в том, чтобы обеспечить классический механизм распознавания и хранение цепей предметной области, а также гибридный такт или классический обучающий курс параметров канала даже для очень больших функциональных пространств.

## <a name="classifier-architecture"></a>Архитектура классификатора

Классификация — это задача защищенного машинного обучения, где целью является определение меток классов $ \{ y_1, y_2, \лдотс, y_d \} $ некоторых примеров данных. "Обучающий набор данных" — это коллекция примеров $ \Маскал{д} = \{ (x, y)} $ с известными предварительно назначенными метками. Здесь $x $ является образцом данных и $y $ — это известная метка с названием «обучающая метка».
По аналогии с традиционными методами, классификация тактов состоит из трех этапов:
- кодирование данных
- подготовка состояния классификатора
- измерения из-за природы вероятностная измерения, эти три шага должны повторяться несколько раз. Как кодирование, так и вычисления состояния классификатора выполняются с помощью *цепей тактов*. Несмотря на то, что канал кодирования обычно управляется данными и без параметров, цепь-классификатор содержит достаточный набор необходимых для изучения параметров. 

В предложенном решении цепь-классификатор состоит из однокубитных поворотов и кубитных поворотов. Для изучения этих параметров используются углы поворота. Шлюзы ротации и управления, управляемые, являются *универсальными* для вычислений тактов. Это означает, что любую одновесовую матрицу можно разложить на достаточно длинный канал, состоящий из таких шлюзов.

В предложенной версии поддерживается только один канал, за которым следует оценка с одной периодичностью.
Таким же решением является тактовая аналогия аппаратного вектора поддержки с низким уровнем полинома.

![Многоуровневый перцептрона и классификатор, ориентированный на канал](~/media/DLvsQCC.png)

Простое проектирование тактовой классификации можно сравнить с традиционным решением на машине поддержки (SVM). Вывод образца данных $x $ в случае SVM выполняется с помощью оптимальной формы ядра $ \сум \ alpha_j k (x_j, x) $, где $k $ является определенной функцией ядра.

Напротив, классификатор такта использует $p прогнозирования (y │ x, U (\сета)) = 〈 U (\сета) x | M | U (\сета) x 〉 $, что похоже на сходство, но технически существенно отличается. Таким образом, при использовании простой кодировки амплитуды $p (y │ x, U (\сета)) $ является квадратичной формой в амплитудах $x $, но коэффициенты этой формы больше не изучены независимо друг от друга. Вместо этого они объединяются из элементов матрицы канала $U (\сета) $, которые, как правило, значительно меньше изученных параметров $ \сета $, чем размерность вектора $x $. Степень полинома $p (y │ x, U (\сета)) $ в исходных функциях может быть увеличена до $2 ^ l $ с использованием тактовой кодировки продукта для $l $ копий $x $.

В нашей архитектуре рассматриваются относительно неполные каналы, которые, следовательно, должны быть *быстро ентанглинг* для захвата всех корреляций между функциями данных во всех диапазонах. Пример наиболее полезного компонента ентанглинг цепи показан на рисунке ниже. Несмотря на то, что канал с этой геометрической схемой состоит только из $3 n + 1 $ Gates, формируемая им матрица единого веса обеспечивает значительный обмен данными между компонентами $2 ^ n $.

![Быстро ентанглинг тактовую цепь на 5 Кубитс (с двумя циклическими слоями).](~/media/5-qubit-qccc.png)

Цепь в приведенном выше примере состоит из 6 однокубитных шлюзов $ (G_1, \лдотс, G_5; G_ {16} ) $ и 10 2-Кубитс Gates $ (G_6, \лдотс, G_ {15} ) $. Предполагая, что каждый из шлюзов определен с одним из этих параметров, у нас есть 16 подученных параметров, а измерение 5-кубит Гильберта пространство — 32. Такая геометрия канала может быть легко обобщена любому $n $-кубит регистру, когда $n $ является нечетным, что дает цепи с $3 n + 1 $ параметрами для $2 ^ n $-многомерного пространства.

## <a name="classifier-training-as-a-supervised-learning-task"></a>Обучение классификатора как контролируемая задача обучения

Обучение модели-классификатора включает в себя поиск оптимальных значений его операционных параметров, что позволяет максимально увеличить среднюю вероятность получения правильных меток обучения по обучающим образцам.
Здесь мы будем беспокоиться только о двух уровнях классификации, т. е. в случае $d = $2 и только два класса с метками $y _1, y_2 $.

> [!NOTE]
> Способ обобщения наших методов с произвольным числом классов заключается в замене Кубитс на кудитс, т. е. единицами такта с $d $ базисными состояниями и двусторонним измерением с $d $-Way.

### <a name="likelihood-as-the-training-goal"></a>Правдоподобие цели обучения

Учитывая проученную цепь тактовой $U (\сета) $, где $ \сета $ является вектором параметров и обозначая окончательную меру на $M $, средняя вероятность правильного вывода метки — $ $ \бегин{алигн} \Маскал{л} (\сета) = \фрак {1} {| \маскал{д} |} \лефт (\ sum_ {(x, y_1) \Ин\маскал{д}} P (M = y_1 | U (\сета) x) + \ sum_ {(x, y_2) \Ин\маскал{д}} P (M = y_2 | U (\сета) x) \ригхт) \енд{алигн} $ $ WHERE $P (M = y | z) $ является вероятностью измерения $y $ в состоянии такта $z $.
Здесь достаточно понять, что функция правдоподобия $ \Маскал{л} (\сета) $ является гладким в $ \сета $ и ее производная в любом $ \ theta_j $ может быть вычислена по сути тем же протоколом такта, который используется для вычисления самой функции правдоподобия. Это позволяет оптимизировать спуск шкалы $ \Маскал{л} (\сета) $ по градиенту.

### <a name="classifier-bias-and-training-score"></a>Смещение классификатора и оценка курса обучения

Учитывая некоторые промежуточные (или окончательные) значения параметров в $ \сета $, нам нужно определить одно вещественное значение $b $ знать как *классификатор смещения* для выполнения вывода. Правило вывода метки работает следующим образом: 
- Пример $x $ назначается меткой $y _2 $ if и только в том случае, если $P (M = y_2 | U (\сета) x) + b > $0,5 (RULE1) (в противном случае назначается метка $y _1 $)

Явно $b $ должен быть в пределах $ (-0,5, + 0,5) $, чтобы быть осмысленным.

Обучающий вариант $ (x, y) \ин \Маскал{д} $ считается неправильной *классификацией* с учетом смещения $b $ если метка, выводимая для $x $ в соответствии с RULE1, на самом деле отличается от $y $. Общее число неправильной классификации — это *показатель обучения* классификатора, заданный в $b $. *Оптимальный* уровень смещения классификатора $b $ свертывает оценку обучения. Это легко увидеть, учитывая предварительно вычисленные оценки вероятности $ \{ P (M = y_2 | U (\сета) x) | (x, *) \Ин\маскал{д} \} $. оптимальное смещение классификатора можно найти по двоичному поиску в интервале $ (-0,5, + 0,5) $, установив максимум $ \ Log_2 (| \маскал{д} |) $ шаги.

### <a name="reference"></a>Справочник

Эта информация должна быть достаточной для начала воспроизведения кода. Однако если вы хотите узнать больше об этой модели, прочитайте первоначальное предложение: [ *"генераторы тактов на основе цепи", Мария Счулд, Алекс Бочаров, Криста Своре и (Nathan виебе*](https://arxiv.org/abs/1804.00633)

В дополнение к образцу кода вы увидите на следующих шагах, вы также можете начать изучение классификации тактов в [этом учебнике](https://github.com/microsoft/QuantumKatas/tree/master/tutorials/QuantumClassification) . 
